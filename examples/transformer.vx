// Mini Transformer Inference in Vortex
// vocab_size=8, d_model=4, n_heads=2, d_ff=8, seq_len=4
// Input tokens: [1, 3, 5, 2]

// --- Helper: dot product of two arrays ---
fn dot(a: [f64], b: [f64]) -> f64 {
    var s = 0.0
    for i in 0..len(a) {
        s += a[i] * b[i]
    }
    return s
}

// --- Helper: add two arrays elementwise ---
fn vec_add(a: [f64], b: [f64]) -> [f64] {
    var result = []
    for i in 0..len(a) {
        result = push(result, a[i] + b[i])
    }
    return result
}

// --- Helper: scale array by scalar ---
fn vec_scale(a: [f64], s: f64) -> [f64] {
    return map(a, |x| x * s)
}

// --- Helper: matrix-vector multiply ---
fn matvec(mat: [f64], v: [f64]) -> [f64] {
    return map(mat, |row| dot(row, v))
}

// --- Embedding lookup ---
fn get_embedding(token_id: i64) -> [f64] {
    let t = token_id * 1.0
    return [t * 0.1, t * 0.2, t * 0.3, t * 0.4]
}

// --- Positional encoding (Taylor approx of sin/cos) ---
fn pos_encoding(pos: i64) -> [f64] {
    let p = pos * 1.0
    let angle0 = p * 0.1
    let sin0 = angle0 - (angle0 ** 3.0) / 6.0
    let cos0 = 1.0 - (angle0 ** 2.0) / 2.0
    let angle1 = p * 0.01
    let sin1 = angle1 - (angle1 ** 3.0) / 6.0
    let cos1 = 1.0 - (angle1 ** 2.0) / 2.0
    return [sin0, cos0, sin1, cos1]
}

// --- Weight matrices (deterministic) ---
fn wq_row(i: i64) -> [f64] {
    if i == 0 { return [0.5, 0.1, 0.0, 0.0] }
    if i == 1 { return [0.1, 0.5, 0.0, 0.0] }
    if i == 2 { return [0.0, 0.0, 0.5, 0.1] }
    return [0.0, 0.0, 0.1, 0.5]
}

fn wk_row(i: i64) -> [f64] {
    if i == 0 { return [0.4, 0.1, 0.0, 0.0] }
    if i == 1 { return [0.1, 0.4, 0.0, 0.0] }
    if i == 2 { return [0.0, 0.0, 0.4, 0.1] }
    return [0.0, 0.0, 0.1, 0.4]
}

fn wv_row(i: i64) -> [f64] {
    if i == 0 { return [0.3, 0.1, 0.0, 0.0] }
    if i == 1 { return [0.1, 0.3, 0.0, 0.0] }
    if i == 2 { return [0.0, 0.0, 0.3, 0.1] }
    return [0.0, 0.0, 0.1, 0.3]
}

fn get_wq() -> [f64] {
    return [wq_row(0), wq_row(1), wq_row(2), wq_row(3)]
}

fn get_wk() -> [f64] {
    return [wk_row(0), wk_row(1), wk_row(2), wk_row(3)]
}

fn get_wv() -> [f64] {
    return [wv_row(0), wv_row(1), wv_row(2), wv_row(3)]
}

fn wo_row(i: i64) -> [f64] {
    if i == 0 { return [0.5, 0.0, 0.0, 0.0] }
    if i == 1 { return [0.0, 0.5, 0.0, 0.0] }
    if i == 2 { return [0.0, 0.0, 0.5, 0.0] }
    return [0.0, 0.0, 0.0, 0.5]
}

fn get_wo() -> [f64] {
    return [wo_row(0), wo_row(1), wo_row(2), wo_row(3)]
}

fn ffn_w1_row(i: i64) -> [f64] {
    let s = (i + 1) * 1.0
    return [s * 0.05, s * 0.04, s * 0.03, s * 0.02]
}

fn ffn_w2_row(i: i64) -> [f64] {
    let s = (i + 1) * 1.0
    return [s*0.02, s*0.03, s*0.02, s*0.03, s*0.01, s*0.02, s*0.01, s*0.02]
}

fn get_ffn_w1() -> [f64] {
    return [ffn_w1_row(0), ffn_w1_row(1), ffn_w1_row(2), ffn_w1_row(3), ffn_w1_row(4), ffn_w1_row(5), ffn_w1_row(6), ffn_w1_row(7)]
}

fn get_ffn_w2() -> [f64] {
    return [ffn_w2_row(0), ffn_w2_row(1), ffn_w2_row(2), ffn_w2_row(3)]
}

fn final_proj_row(i: i64) -> [f64] {
    let s = (i + 1) * 1.0
    return [s * 0.1, s * 0.05, s * 0.08, s * 0.06]
}

fn get_final_proj() -> [f64] {
    return [final_proj_row(0), final_proj_row(1), final_proj_row(2), final_proj_row(3), final_proj_row(4), final_proj_row(5), final_proj_row(6), final_proj_row(7)]
}

// --- Single-head attention ---
fn single_head_attn(q_vec: [f64], k_vecs: [f64], v_vecs: [f64], d_head: i64) -> [f64] {
    let scale = (d_head * 1.0) ** 0.5
    var scores = []
    for i in 0..len(k_vecs) {
        let s = dot(q_vec, k_vecs[i]) / scale
        scores = push(scores, s)
    }
    let weights = softmax(scores)
    let d = len(q_vec)
    var result = []
    for j in 0..d {
        result = push(result, 0.0)
    }
    for i in 0..len(v_vecs) {
        let w = weights[i]
        let v = v_vecs[i]
        var new_result = []
        for j in 0..d {
            new_result = push(new_result, result[j] + v[j] * w)
        }
        result = new_result
    }
    return result
}

// --- Multi-head attention for a single position ---
fn multi_head_attn(x_seq: [f64], pos: i64) -> [f64] {
    let wq = get_wq()
    let wk = get_wk()
    let wv = get_wv()
    let wo = get_wo()

    let x = x_seq[pos]
    let q_full = matvec(wq, x)

    let seq_len = len(x_seq)
    var k_full_seq = []
    var v_full_seq = []
    for i in 0..seq_len {
        k_full_seq = push(k_full_seq, matvec(wk, x_seq[i]))
        v_full_seq = push(v_full_seq, matvec(wv, x_seq[i]))
    }

    // Head 0: dims [0,1], Head 1: dims [2,3]
    let q_h0 = [q_full[0], q_full[1]]
    let q_h1 = [q_full[2], q_full[3]]

    var k_h0 = []
    var k_h1 = []
    var v_h0 = []
    var v_h1 = []
    for i in 0..seq_len {
        let kf = k_full_seq[i]
        let vf = v_full_seq[i]
        k_h0 = push(k_h0, [kf[0], kf[1]])
        k_h1 = push(k_h1, [kf[2], kf[3]])
        v_h0 = push(v_h0, [vf[0], vf[1]])
        v_h1 = push(v_h1, [vf[2], vf[3]])
    }

    let out_h0 = single_head_attn(q_h0, k_h0, v_h0, 2)
    let out_h1 = single_head_attn(q_h1, k_h1, v_h1, 2)

    let concat = [out_h0[0], out_h0[1], out_h1[0], out_h1[1]]
    return matvec(wo, concat)
}

// --- Feed-forward network: W2 * GELU(W1 * x) ---
fn ffn(x: [f64]) -> [f64] {
    let w1 = get_ffn_w1()
    let w2 = get_ffn_w2()
    let hidden = matvec(w1, x)
    let activated = map(hidden, |h| gelu(h))
    return matvec(w2, activated)
}

// --- Transformer block for one position ---
fn transformer_block(x_seq: [f64], pos: i64) -> [f64] {
    let x = x_seq[pos]

    // Multi-head self-attention + residual connection
    let attn_out = multi_head_attn(x_seq, pos)
    let post_attn = vec_add(x, attn_out)

    // Layer normalization after attention
    let normed = layer_norm(post_attn)

    // Feed-forward network + residual connection
    let ff_out = ffn(normed)
    let post_ff = vec_add(normed, ff_out)

    // Final layer normalization
    return layer_norm(post_ff)
}

// --- Main ---
fn main() {
    println("=== Mini Transformer Inference ===")
    println("")

    let tokens = [1, 3, 5, 2]
    println(format("Input tokens: {}", to_string(tokens)))

    // Step 1: Token embedding + positional encoding
    println("Step 1: Embedding + positional encoding")
    var x_seq = []
    for i in 0..4 {
        let tok = tokens[i]
        let emb = get_embedding(tok)
        let pe = pos_encoding(i)
        let combined = vec_add(emb, pe)
        x_seq = push(x_seq, combined)
    }

    for i in 0..4 {
        println(format("  pos {}: {}", to_string(i), to_string(x_seq[i])))
    }

    // Step 2: Run transformer block on each position
    println("")
    println("Step 2: Transformer block (attention + FFN + layer norm)")
    var output_seq = []
    for i in 0..4 {
        let out = transformer_block(x_seq, i)
        output_seq = push(output_seq, out)
    }

    for i in 0..4 {
        println(format("  pos {}: {}", to_string(i), to_string(output_seq[i])))
    }

    // Step 3: Final projection + softmax for next-token prediction
    println("")
    println("Step 3: Output projection + softmax (last position)")
    let last_hidden = output_seq[3]
    let proj = get_final_proj()
    let logits = matvec(proj, last_hidden)
    println(format("  Logits: {}", to_string(logits)))

    let probs = softmax(logits)
    println(format("  Probabilities: {}", to_string(probs)))

    let prob_sum = sum(probs)
    println(format("  Sum of probs: {}", to_string(prob_sum)))

    // Find most likely next token
    var max_prob = 0.0
    var max_idx = 0
    for i in 0..len(probs) {
        let p = probs[i]
        if p > max_prob {
            max_prob = p
            max_idx = i
        }
    }
    println(format("  Most likely next token: {} (prob: {})", to_string(max_idx), to_string(max_prob)))

    // Validate probabilities sum to ~1.0
    assert(prob_sum > 0.99, "probabilities should sum to ~1.0")
    assert(prob_sum < 1.01, "probabilities should sum to ~1.0")
    println("")
    println("All assertions passed! Transformer inference complete.")
}
