// Liquid Time-Constant Networks: Neural ODE with adaptive dynamics
// TERRIBLE fit for static computation graphs (PyTorch):
// - Time constants adapt PER INPUT (dynamic graph structure)
// - ODE integration requires variable step sizes
// - PyTorch: requires torchdiffeq + custom adjoint method
// - Vortex: native ODE solvers + liquid cell builtins

fn abs_f(x: f64) -> f64 {
    if x < 0.0 { return 0.0 - x }
    return x
}

fn print_array(name: str, arr: [f64]) {
    let n = len(arr)
    var s = format("{}: [", name)
    for i in 0..n {
        if i > 0 { s = format("{}, ", s) }
        s = format("{}{}", s, to_string(arr[i]))
    }
    s = format("{}]", s)
    println(s)
}

// --- Sigmoid approximation ---
fn sigmoid(x: f64) -> f64 {
    if x > 5.0 { return 1.0 }
    if x < -5.0 { return 0.0 }
    return 0.5 + x * 0.2
}

// --- Tanh approximation ---
fn tanh_approx(x: f64) -> f64 {
    if x > 3.0 { return 1.0 }
    if x < -3.0 { return -1.0 }
    return x * (1.0 - x * x / 9.0)
}

// --- Manual liquid cell: dh/dt = (-h + tanh(Wh*h + Wx*x)) / tau(x) ---
fn manual_liquid_step(
    h: [f64], x: [f64],
    w_h: [f64], w_x: [f64],
    tau: [f64], dt: f64,
    n_hidden: int, n_input: int
) -> [f64] {
    // Compute adaptive time constants: tau_eff = sigmoid(tau)
    var tau_eff = []
    for i in 0..n_hidden {
        tau_eff = push(tau_eff, sigmoid(tau[i]) + 0.1)
    }

    // Compute activation: tanh(Wh @ h + Wx @ x)
    var activation = []
    for i in 0..n_hidden {
        var val = 0.0
        for j in 0..n_hidden {
            val = val + w_h[i * n_hidden + j] * h[j]
        }
        for j in 0..n_input {
            val = val + w_x[i * n_input + j] * x[j]
        }
        activation = push(activation, tanh_approx(val))
    }

    // ODE step: h_new = h + dt * (-h + activation) / tau_eff
    var h_new = []
    for i in 0..n_hidden {
        let dhdt = (0.0 - h[i] + activation[i]) / tau_eff[i]
        h_new = push(h_new, h[i] + dt * dhdt)
    }
    return h_new
}

// --- Adaptive step size: double/halve dt based on rate of change ---
fn adaptive_liquid_solve(
    h0: [f64], inputs: [f64],
    w_h: [f64], w_x: [f64], tau: [f64],
    n_hidden: int, n_input: int,
    n_steps: int
) -> [f64] {
    var h = h0
    var trajectory = []
    trajectory = push(trajectory, h)

    for t in 0..n_steps {
        let x = inputs[t]

        // Take one full step with dt=0.1
        let h_full = manual_liquid_step(h, x, w_h, w_x, tau, 0.1, n_hidden, n_input)

        // Take two half steps with dt=0.05
        let h_half1 = manual_liquid_step(h, x, w_h, w_x, tau, 0.05, n_hidden, n_input)
        let h_half2 = manual_liquid_step(h_half1, x, w_h, w_x, tau, 0.05, n_hidden, n_input)

        // Estimate error: difference between methods
        var err = 0.0
        for i in 0..n_hidden {
            err = err + abs_f(h_full[i] - h_half2[i])
        }

        // Use the more accurate (half-step) result
        h = h_half2
        trajectory = push(trajectory, h)
    }
    return trajectory
}

fn main() {
    println("========================================")
    println("  Liquid Neural ODE Demo")
    println("========================================")
    println("")
    println("Liquid Time-Constant Networks: time constants adapt per input.")
    println("PyTorch: torchdiffeq + custom adjoint + detach hacks")
    println("Vortex: native ODE solvers + liquid_cell/cfc_cell builtins")
    println("")

    let n_hidden = 2
    let n_input = 2

    // Initial hidden state
    let h0 = [0.0, 0.0]

    // Weight matrices (flattened)
    let w_h = [0.1, -0.2, 0.3, 0.1]
    let w_x = [0.5, 0.3, 0.2, 0.4]
    let tau = [1.0, 1.5]

    // --- Step 1: Compare manual vs builtin liquid cell ---
    println("--- Step 1: Liquid Cell Comparison ---")
    let x_test = [1.0, 0.5]

    let manual_dh = manual_liquid_step(h0, x_test, w_h, w_x, tau, 0.01, n_hidden, n_input)
    print_array("  manual step", manual_dh)

    let builtin_dh = liquid_cell(h0, x_test, w_h, w_x, tau, n_hidden, n_input)
    print_array("  builtin dh/dt", builtin_dh)
    println("")

    // --- Step 2: Euler vs RK4 integration ---
    println("--- Step 2: ODE Integration (Euler vs RK4) ---")
    let h_euler = ode_solve_euler(h0, 0.0, 1.0, 100, h0, x_test, w_h, w_x, tau, n_hidden, n_input)
    let h_rk4 = ode_solve_rk4(h0, 0.0, 1.0, 100, h0, x_test, w_h, w_x, tau, n_hidden, n_input)
    print_array("  Euler h(1.0)", h_euler)
    print_array("  RK4   h(1.0)", h_rk4)
    let euler_rk4_diff = abs_f(h_euler[0] - h_rk4[0]) + abs_f(h_euler[1] - h_rk4[1])
    println(format("  |Euler - RK4| = {} (RK4 is 4th-order accurate)", to_string(euler_rk4_diff)))
    println("")

    // --- Step 3: CfC (Closed-form Continuous-time) approximation ---
    println("--- Step 3: CfC Closed-Form Approximation ---")
    let h_cfc = cfc_cell(h0, x_test, w_h, w_x, tau, 1.0, n_hidden, n_input)
    print_array("  CfC    h(1.0)", h_cfc)
    println("  CfC gives an analytical approximation (no ODE steps needed)")
    println("")

    // --- Step 4: Process a time series with adaptive dynamics ---
    println("--- Step 4: Time Series (adaptive dynamics per input) ---")
    let inputs = [[1.0, 0.5], [0.8, 0.3], [-0.2, 0.9], [0.5, -0.4], [0.1, 0.1],
                  [1.5, -0.2], [-0.8, 0.6], [0.3, 0.3], [-0.1, -0.5], [0.7, 0.8]]
    var h = [0.0, 0.0]
    for t in 0..10 {
        let xi = inputs[t]
        h = cfc_cell(h, xi, w_h, w_x, tau, 0.1, n_hidden, n_input)
        println(format("  t={}: input=[{}, {}] -> h=[{}, {}]",
            t, to_string(xi[0]), to_string(xi[1]), to_string(h[0]), to_string(h[1])))
    }
    println("")

    // --- Step 5: Manual adaptive step size integration ---
    println("--- Step 5: Adaptive Step Size (error estimation) ---")
    let trajectory = adaptive_liquid_solve(
        h0, inputs, w_h, w_x, tau, n_hidden, n_input, 10
    )
    println(format("  Trajectory length: {} states", len(trajectory)))
    let h_final = trajectory[10]
    print_array("  Final state", h_final)
    println("  (Adaptive stepping uses Richardson extrapolation for error control)")
    println("")

    // --- Summary ---
    println("--- Why Liquid Neural ODEs need Vortex ---")
    println("  1. Time constants are INPUT-DEPENDENT (dynamic compute graph)")
    println("  2. ODE solvers need adaptive step sizes (variable-length loops)")
    println("  3. CfC gives closed-form solution (no iterative solving)")
    println("  4. All three methods available as native builtins")
    println("  PyTorch requires: torchdiffeq, torch.autograd.Function, .detach() hacks")
    println("")

    println("========================================")
    println("  Liquid Neural ODE demo complete!")
    println("========================================")
}
