// GPT Model Definition in Vortex
// Defines model config, weight initialization, and forward pass

// --- Vector helpers ---
fn dot(a: [f64], b: [f64]) -> f64 {
    var s = 0.0
    var i = 0
    while i < len(a) {
        s = s + a[i] * b[i]
        i = i + 1
    }
    return s
}

fn vec_add(a: [f64], b: [f64]) -> [f64] {
    var result = []
    var i = 0
    while i < len(a) {
        result = push(result, a[i] + b[i])
        i = i + 1
    }
    return result
}

fn vec_scale(a: [f64], s: f64) -> [f64] {
    var result = []
    var i = 0
    while i < len(a) {
        result = push(result, a[i] * s)
        i = i + 1
    }
    return result
}

fn matvec(mat: [f64], v: [f64]) -> [f64] {
    var result = []
    var i = 0
    while i < len(mat) {
        result = push(result, dot(mat[i], v))
        i = i + 1
    }
    return result
}

fn zeros(n: i64) -> [f64] {
    var result = []
    var i = 0
    while i < n {
        result = push(result, 0.0)
        i = i + 1
    }
    return result
}

// --- Model configuration ---
fn gpt_config(vocab_size: i64, d_model: i64, n_heads: i64, n_layers: i64, d_ff: i64, max_seq_len: i64) -> any {
    var config = hashmap()
    config = hashmap_insert(config, "vocab_size", vocab_size)
    config = hashmap_insert(config, "d_model", d_model)
    config = hashmap_insert(config, "n_heads", n_heads)
    config = hashmap_insert(config, "n_layers", n_layers)
    config = hashmap_insert(config, "d_ff", d_ff)
    config = hashmap_insert(config, "max_seq_len", max_seq_len)
    return config
}

fn cfg_get(config: any, key: str) -> i64 {
    return unwrap(hashmap_get(config, key))
}

// Deterministic pseudo-random weight init
fn det_weight(seed: i64) -> f64 {
    let x = ((seed * 1103515245 + 12345) % 65536) * 1.0
    return (x / 65536.0 - 0.5) * 0.4
}

// Initialize a weight matrix (rows x cols) with deterministic values
fn init_matrix(rows: i64, cols: i64, seed: i64) -> [f64] {
    var mat = []
    var r = 0
    while r < rows {
        var row = []
        var c = 0
        while c < cols {
            row = push(row, det_weight(seed + r * cols + c))
            c = c + 1
        }
        mat = push(mat, row)
        r = r + 1
    }
    return mat
}

fn init_vector(n: i64, seed: i64) -> [f64] {
    var v = []
    var i = 0
    while i < n {
        v = push(v, det_weight(seed + i))
        i = i + 1
    }
    return v
}

// Initialize all GPT weights
fn init_weights(config: any) -> any {
    let d = cfg_get(config, "d_model")
    let v = cfg_get(config, "vocab_size")
    let ff = cfg_get(config, "d_ff")
    let seq = cfg_get(config, "max_seq_len")

    var weights = hashmap()

    // Token embeddings: vocab_size x d_model
    weights = hashmap_insert(weights, "tok_emb", init_matrix(v, d, 42))

    // Position embeddings: max_seq_len x d_model
    weights = hashmap_insert(weights, "pos_emb", init_matrix(seq, d, 137))

    // Layer 0 weights
    weights = hashmap_insert(weights, "wq", init_matrix(d, d, 200))
    weights = hashmap_insert(weights, "wk", init_matrix(d, d, 300))
    weights = hashmap_insert(weights, "wv", init_matrix(d, d, 400))
    weights = hashmap_insert(weights, "wo", init_matrix(d, d, 500))

    // FFN weights
    weights = hashmap_insert(weights, "ffn_w1", init_matrix(ff, d, 600))
    weights = hashmap_insert(weights, "ffn_w2", init_matrix(d, ff, 700))
    weights = hashmap_insert(weights, "ffn_b1", init_vector(ff, 800))
    weights = hashmap_insert(weights, "ffn_b2", init_vector(d, 900))

    // Layer norm parameters (gamma=1, beta=0 for simplicity)
    weights = hashmap_insert(weights, "ln1_gamma", [1.0, 1.0, 1.0, 1.0])
    weights = hashmap_insert(weights, "ln1_beta", [0.0, 0.0, 0.0, 0.0])
    weights = hashmap_insert(weights, "ln2_gamma", [1.0, 1.0, 1.0, 1.0])
    weights = hashmap_insert(weights, "ln2_beta", [0.0, 0.0, 0.0, 0.0])

    // Final output projection: d_model -> vocab_size
    weights = hashmap_insert(weights, "out_proj", init_matrix(v, d, 1000))

    return weights
}

fn w_get(weights: any, key: str) -> any {
    return unwrap(hashmap_get(weights, key))
}

// Apply layer norm with gamma and beta
fn apply_layer_norm(x: [f64], gamma: [f64], beta: [f64]) -> [f64] {
    let normed = layer_norm(x)
    var result = []
    var i = 0
    while i < len(x) {
        result = push(result, normed[i] * gamma[i] + beta[i])
        i = i + 1
    }
    return result
}

// Single-head attention (causal)
fn causal_attention(q_vec: [f64], k_vecs: [f64], v_vecs: [f64], pos: i64, d_head: i64) -> [f64] {
    let scale = (d_head * 1.0) ** 0.5
    var scores = []
    // Only attend to positions <= pos (causal mask)
    var i = 0
    while i <= pos {
        let s = dot(q_vec, k_vecs[i]) / scale
        scores = push(scores, s)
        i = i + 1
    }
    let weights_attn = softmax(scores)
    var result = zeros(len(q_vec))
    var j = 0
    while j <= pos {
        let w = weights_attn[j]
        let v = v_vecs[j]
        var new_r = []
        var k = 0
        while k < len(result) {
            new_r = push(new_r, result[k] + v[k] * w)
            k = k + 1
        }
        result = new_r
        j = j + 1
    }
    return result
}

// Forward pass: tokens -> logits
fn gpt_forward(tokens: [i64], weights: any, config: any) -> [f64] {
    let d = cfg_get(config, "d_model")
    let v = cfg_get(config, "vocab_size")
    let seq_len = len(tokens)

    let tok_emb = w_get(weights, "tok_emb")
    let pos_emb = w_get(weights, "pos_emb")
    let wq = w_get(weights, "wq")
    let wk = w_get(weights, "wk")
    let wv = w_get(weights, "wv")
    let wo = w_get(weights, "wo")
    let ffn_w1 = w_get(weights, "ffn_w1")
    let ffn_w2 = w_get(weights, "ffn_w2")
    let ffn_b1 = w_get(weights, "ffn_b1")
    let ffn_b2 = w_get(weights, "ffn_b2")
    let ln1_gamma = w_get(weights, "ln1_gamma")
    let ln1_beta = w_get(weights, "ln1_beta")
    let ln2_gamma = w_get(weights, "ln2_gamma")
    let ln2_beta = w_get(weights, "ln2_beta")
    let out_proj = w_get(weights, "out_proj")

    // 1. Embedding: token + position
    var x_seq = []
    var i = 0
    while i < seq_len {
        let tok_id = tokens[i]
        let emb = vec_add(tok_emb[tok_id], pos_emb[i])
        x_seq = push(x_seq, emb)
        i = i + 1
    }

    // 2. Compute Q, K, V for all positions
    var q_all = []
    var k_all = []
    var v_all = []
    var p = 0
    while p < seq_len {
        q_all = push(q_all, matvec(wq, x_seq[p]))
        k_all = push(k_all, matvec(wk, x_seq[p]))
        v_all = push(v_all, matvec(wv, x_seq[p]))
        p = p + 1
    }

    // 3. Self-attention + residual + layer norm for each position
    var post_attn = []
    var p2 = 0
    while p2 < seq_len {
        let attn_out = causal_attention(q_all[p2], k_all, v_all, p2, d)
        let projected = matvec(wo, attn_out)
        let residual = vec_add(x_seq[p2], projected)
        let normed = apply_layer_norm(residual, ln1_gamma, ln1_beta)
        post_attn = push(post_attn, normed)
        p2 = p2 + 1
    }

    // 4. FFN + residual + layer norm for each position
    var post_ffn = []
    var p3 = 0
    while p3 < seq_len {
        let hidden = vec_add(matvec(ffn_w1, post_attn[p3]), ffn_b1)
        let activated = map(hidden, |h| gelu(h))
        let ff_out = vec_add(matvec(ffn_w2, activated), ffn_b2)
        let residual2 = vec_add(post_attn[p3], ff_out)
        let normed2 = apply_layer_norm(residual2, ln2_gamma, ln2_beta)
        post_ffn = push(post_ffn, normed2)
        p3 = p3 + 1
    }

    // 5. Output projection on last position -> logits
    let last = post_ffn[seq_len - 1]
    let logits = matvec(out_proj, last)

    return logits
}

// Argmax of an array
fn argmax(arr: [f64]) -> i64 {
    var best = 0
    var best_val = arr[0]
    var i = 1
    while i < len(arr) {
        if arr[i] > best_val {
            best_val = arr[i]
            best = i
        }
        i = i + 1
    }
    return best
}

fn main() {
    println("=== GPT Model Definition ===")
    println("")

    let config = gpt_config(8, 4, 1, 1, 8, 8)
    println(format("Config: vocab={}, d_model={}, heads={}, layers={}, d_ff={}, seq_len={}",
        cfg_get(config, "vocab_size"), cfg_get(config, "d_model"),
        cfg_get(config, "n_heads"), cfg_get(config, "n_layers"),
        cfg_get(config, "d_ff"), cfg_get(config, "max_seq_len")))

    let weights = init_weights(config)
    println(format("Initialized {} weight matrices", hashmap_len(weights)))

    // Test forward pass
    let tokens = [1, 3, 5, 2]
    println(format("Input tokens: {}", to_string(tokens)))

    let logits = gpt_forward(tokens, weights, config)
    println(format("Output logits: {}", to_string(logits)))

    let probs = softmax(logits)
    println(format("Probabilities: {}", to_string(probs)))

    let next_token = argmax(probs)
    println(format("Predicted next token: {}", next_token))

    let prob_sum = sum(probs)
    assert(prob_sum > 0.99, "probs must sum to ~1")
    assert(prob_sum < 1.01, "probs must sum to ~1")
    println("")
    println("GPT model forward pass complete!")
}
