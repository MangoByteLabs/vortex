// adaptive_depth.vx — Variable-Depth Reasoning
//
// Tokens/inputs independently decide when to stop processing.
// PyTorch cannot do this because computation graphs are static —
// every token must pass through every layer even if it "knows the answer" early.
//
// Based on the Adaptive Computation Time (ACT) / Universal Transformer idea,
// but implemented natively so the halting decision is a first-class part
// of the language runtime, not a workaround bolted onto a fixed graph.
//
// Layout conventions
//   layer  : flat [f64] of dim*dim weights (row-major)
//   input  : [f64] of length dim
//   batch  : inputs encoded as [String] — each entry is space-separated floats
//   result : [String] — outputs followed by metadata strings

// ─────────────────────────────────────────────────────────────
// Section 1: Math helpers
// ─────────────────────────────────────────────────────────────

fn abs_f(x: f64) -> f64 {
    if x < 0.0 { 0.0 - x } else { x }
}

fn exp_approx(x: f64) -> f64 {
    // Pade approximant valid for x in [-5, 5]; clamp outside
    var cx = x
    if cx > 5.0 { cx = 5.0 }
    if cx < 0.0 - 5.0 { cx = 0.0 - 5.0 }
    var num = 1.0 + cx + cx * cx * 0.5 + cx * cx * cx * 0.16666667
    var den = 1.0 - cx + cx * cx * 0.5 - cx * cx * cx * 0.16666667
    if den < 0.0001 { den = 0.0001 }
    num / den
}

fn sigmoid(x: f64) -> f64 {
    1.0 / (1.0 + exp_approx(0.0 - x))
}

fn relu(x: f64) -> f64 {
    if x > 0.0 { x } else { 0.0 }
}

fn sqrt_approx(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 }
    var g = x * 0.5
    var i = 0
    while i < 20 {
        g = (g + x / g) * 0.5
        i = i + 1
    }
    g
}

// ─────────────────────────────────────────────────────────────
// Section 2: Layer operations
// ─────────────────────────────────────────────────────────────

// layer_new — initialise a dim×dim weight matrix with pseudo-random values.
// Uses a simple LCG seeded on the dimension; weights are in (-0.5, 0.5).
fn layer_new(dim: i64) -> [f64] {
    var weights = []
    var size = dim * dim
    var seed = (dim * 6364136223846793005 + 1442695040888963407) % 2147483647
    var i = 0
    while i < size {
        seed = (seed * 6364136223846793005 + 1442695040888963407) % 2147483647
        // bring into (0,1) then shift to (-0.5, 0.5)
        var raw = float(seed % 100000) / 100000.0 - 0.5
        // scale by 1/sqrt(dim) for sensible activations
        var scale = 1.0 / sqrt_approx(float(dim))
        weights = push(weights, raw * scale)
        i = i + 1
    }
    weights
}

// layer_forward — dense matrix-vector multiply: out = relu(W * input)
// weights: row-major dim×dim, input: length dim
fn layer_forward(weights: [f64], input: [f64], dim: i64) -> [f64] {
    var output = []
    var row = 0
    while row < dim {
        var acc = 0.0
        var col = 0
        while col < dim {
            acc = acc + weights[row * dim + col] * input[col]
            col = col + 1
        }
        output = push(output, relu(acc))
        row = row + 1
    }
    output
}

// halting_score — scalar halt probability: sigmoid(mean(hidden))
// Values near 1.0 mean "I am confident enough; stop processing."
fn halting_score(hidden: [f64]) -> f64 {
    var d = len(hidden)
    if d == 0 { return 0.0 }
    var s = 0.0
    var i = 0
    while i < d {
        s = s + hidden[i]
        i = i + 1
    }
    sigmoid(s / float(d))
}

// ─────────────────────────────────────────────────────────────
// Section 3: Encoding helpers (f64 array <-> String)
// ─────────────────────────────────────────────────────────────

// encode_vec — pack [f64] into a space-separated String
fn encode_vec(v: [f64]) -> String {
    var n = len(v)
    if n == 0 { return "" }
    var s = to_string(v[0])
    var i = 1
    while i < n {
        s = s + " " + to_string(v[i])
        i = i + 1
    }
    s
}

// decode_vec — unpack space-separated String back to [f64]
// Assumes exactly `dim` tokens; extra tokens are ignored.
fn decode_vec(s: String, dim: i64) -> [f64] {
    var result = []
    var current = ""
    var idx = 0
    var slen = len(s)
    while idx < slen {
        var ch = str_char_at(s, idx)
        if ch == " " {
            if len(current) > 0 {
                result = push(result, float(current))
                current = ""
            }
        } else {
            current = current + ch
        }
        idx = idx + 1
    }
    if len(current) > 0 {
        result = push(result, float(current))
    }
    // pad with zeros if shorter than dim
    while len(result) < dim {
        result = push(result, 0.0)
    }
    result
}

// ─────────────────────────────────────────────────────────────
// Section 4: Adaptive forward pass
// ─────────────────────────────────────────────────────────────

// adaptive_forward — process input through layers, halting early when confident.
//
// layers   : [String] where each entry is the encoded weight vector for one layer
// input    : [f64] initial hidden state
// dim      : hidden dimension
// max_depth: maximum number of layers to apply
//
// Returns [String]:
//   [0..dim-1] : output vector (space-separated floats, one String per dim element)
//   [dim]      : depth_used as string
//   [dim+1]    : confidence as string
fn adaptive_forward(layers: [String], input: [f64], dim: i64, max_depth: i64) -> [String] {
    var hidden = input
    var depth = 0
    var confidence = 0.0
    var halted = false
    var n_layers = len(layers)
    var limit = max_depth
    if n_layers < limit { limit = n_layers }

    while depth < limit {
        if halted { depth = depth + 1 }
        if halted == false {
            // decode this layer's weights from its String encoding
            var layer_weights = decode_vec(layers[depth], dim * dim)
            hidden = layer_forward(layer_weights, hidden, dim)
            confidence = halting_score(hidden)
            depth = depth + 1
            if confidence > 0.9 {
                halted = true
            }
        }
    }

    // pack output
    var out = []
    var i = 0
    while i < dim {
        out = push(out, to_string(hidden[i]))
        i = i + 1
    }
    out = push(out, to_string(depth))
    out = push(out, to_string(confidence))
    out
}

// ─────────────────────────────────────────────────────────────
// Section 5: Batch processing — each input exits at its own depth
// ─────────────────────────────────────────────────────────────

// batch_adaptive_forward — run adaptive_forward over a batch of inputs.
//
// inputs : [String] where each entry is an encoded [f64] vector of length dim
//
// Returns [String]: for each input, one entry "depth:confidence"
fn batch_adaptive_forward(layers: [String], inputs: [String], dim: i64, max_depth: i64) -> [String] {
    var results = []
    var b = len(inputs)
    var i = 0
    while i < b {
        var input_vec = decode_vec(inputs[i], dim)
        var fwd = adaptive_forward(layers, input_vec, dim, max_depth)
        // fwd[dim] = depth_str, fwd[dim+1] = confidence_str
        var depth_s = fwd[dim]
        var conf_s = fwd[dim + 1]
        results = push(results, depth_s + ":" + conf_s)
        i = i + 1
    }
    results
}

// ─────────────────────────────────────────────────────────────
// Section 6: Pondering cost (regularisation)
// ─────────────────────────────────────────────────────────────

// ponder_cost — L2 regularisation on depth usage.
// Encourages the model to halt early; penalises deep pondering.
// cost = (1/N) * sum(depth_i^2) / max_depth^2
fn ponder_cost(depths: [i64]) -> f64 {
    var n = len(depths)
    if n == 0 { return 0.0 }
    var total = 0.0
    var i = 0
    while i < n {
        var d = float(depths[i])
        total = total + d * d
        i = i + 1
    }
    total / float(n)
}

// ─────────────────────────────────────────────────────────────
// Section 7: Training step
// ─────────────────────────────────────────────────────────────

// vec_mse — mean-squared error between two f64 vectors
fn vec_mse(a: [f64], b: [f64]) -> f64 {
    var n = len(a)
    if n == 0 { return 0.0 }
    var s = 0.0
    var i = 0
    while i < n {
        var delta = a[i] - b[i]
        s = s + delta * delta
        i = i + 1
    }
    s / float(n)
}

// finite_diff_grad — estimate gradient of layer output wrt weights via finite differences
// Returns gradient as flat [f64] same shape as weights
fn finite_diff_grad(weights: [f64], input: [f64], target: [f64], dim: i64, eps: f64) -> [f64] {
    var n = len(weights)
    var grads = []
    var i = 0
    while i < n {
        // perturb +eps
        var wp = []
        var j = 0
        while j < n {
            if j == i { wp = push(wp, weights[j] + eps) }
            else       { wp = push(wp, weights[j]) }
            j = j + 1
        }
        var out_p = layer_forward(wp, input, dim)
        var loss_p = vec_mse(out_p, target)

        // perturb -eps
        var wm = []
        j = 0
        while j < n {
            if j == i { wm = push(wm, weights[j] - eps) }
            else       { wm = push(wm, weights[j]) }
            j = j + 1
        }
        var out_m = layer_forward(wm, input, dim)
        var loss_m = vec_mse(out_m, target)

        grads = push(grads, (loss_p - loss_m) / (2.0 * eps))
        i = i + 1
    }
    grads
}

// sgd_step — one SGD update on a weight vector
fn sgd_step(weights: [f64], grads: [f64], lr: f64) -> [f64] {
    var n = len(weights)
    var updated = []
    var i = 0
    while i < n {
        updated = push(updated, weights[i] - lr * grads[i])
        i = i + 1
    }
    updated
}

// adaptive_train_step — one training step.
//
// 1. Run adaptive_forward to get output and depth used.
// 2. Compute MSE loss against target.
// 3. Add ponder_cost penalty for the depth used.
// 4. Update only the layers that were actually used (finite-diff gradients).
//
// Returns [String]: updated layers with same encoding, plus
//   last entry: "loss:<mse>|depth:<d>|total_loss:<mse+ponder>"
fn adaptive_train_step(layers: [String], input: [f64], target: [f64], dim: i64, max_depth: i64, lr: f64) -> [String] {
    var fwd = adaptive_forward(layers, input, dim, max_depth)
    var output = []
    var k = 0
    while k < dim {
        output = push(output, float(fwd[k]))
        k = k + 1
    }
    var depth_used = int(fwd[dim])
    var mse = vec_mse(output, target)

    var depths_arr = [depth_used]
    var pc = ponder_cost(depths_arr)
    var total_loss = mse + 0.01 * pc

    // update used layers only
    var new_layers = []
    var li = 0
    var n_layers = len(layers)
    while li < n_layers {
        if li < depth_used {
            var w = decode_vec(layers[li], dim * dim)
            var g = finite_diff_grad(w, input, target, dim, 0.001)
            var w_new = sgd_step(w, g, lr)
            new_layers = push(new_layers, encode_vec(w_new))
        } else {
            new_layers = push(new_layers, layers[li])
        }
        li = li + 1
    }

    var summary = "loss:" + to_string(mse) + "|depth:" + to_string(depth_used) + "|total:" + to_string(total_loss)
    new_layers = push(new_layers, summary)
    new_layers
}

// ─────────────────────────────────────────────────────────────
// Section 8: Demo
// ─────────────────────────────────────────────────────────────

fn main() {
    var nl = str_from_bytes([10])
    var tab = str_from_bytes([9])

    println("=== Adaptive Depth Reasoning Demo ===" + nl)
    println("Variable-depth computation: each input halts when it is confident." + nl)
    println("Static frameworks process every token through every layer." + nl)
    println("Vortex makes early-exit a first-class language feature." + nl)

    var dim = 4
    var max_depth = 4

    // Build 4 layers, encode weights as strings
    var layers = []
    var li = 0
    while li < max_depth {
        var w = layer_new(dim)
        layers = push(layers, encode_vec(w))
        li = li + 1
    }
    println("Built " + to_string(max_depth) + " layers, hidden dim = " + to_string(dim) + nl)

    // Craft inputs designed to halt at different depths
    // input_a: small activations — needs many layers
    var input_a = [0.01, 0.02, 0.01, 0.03]
    // input_b: medium activations
    var input_b = [0.3, 0.5, 0.2, 0.4]
    // input_c: strong activations — should halt early
    var input_c = [1.8, 2.1, 1.6, 2.3]
    // input_d: very strong — almost certain to halt at layer 1
    var input_d = [3.5, 4.0, 3.8, 4.2]

    println("--- Per-input adaptive halting ---" + nl)

    var fwd_a = adaptive_forward(layers, input_a, dim, max_depth)
    println(tab + "Input A (weak):   depth=" + fwd_a[dim] + "  confidence=" + fwd_a[dim + 1])

    var fwd_b = adaptive_forward(layers, input_b, dim, max_depth)
    println(tab + "Input B (medium): depth=" + fwd_b[dim] + "  confidence=" + fwd_b[dim + 1])

    var fwd_c = adaptive_forward(layers, input_c, dim, max_depth)
    println(tab + "Input C (strong): depth=" + fwd_c[dim] + "  confidence=" + fwd_c[dim + 1])

    var fwd_d = adaptive_forward(layers, input_d, dim, max_depth)
    println(tab + "Input D (very):   depth=" + fwd_d[dim] + "  confidence=" + fwd_d[dim + 1])

    println(nl + "--- Batch adaptive forward ---" + nl)
    var batch_inputs = []
    batch_inputs = push(batch_inputs, encode_vec(input_a))
    batch_inputs = push(batch_inputs, encode_vec(input_b))
    batch_inputs = push(batch_inputs, encode_vec(input_c))
    batch_inputs = push(batch_inputs, encode_vec(input_d))

    var batch_results = batch_adaptive_forward(layers, batch_inputs, dim, max_depth)
    var bi = 0
    while bi < len(batch_results) {
        println(tab + "Batch[" + to_string(bi) + "] depth:confidence = " + batch_results[bi])
        bi = bi + 1
    }

    println(nl + "--- Ponder cost regularisation ---" + nl)
    var depths_demo = [int(fwd_a[dim]), int(fwd_b[dim]), int(fwd_c[dim]), int(fwd_d[dim])]
    var pc = ponder_cost(depths_demo)
    println(tab + "Ponder cost = " + to_string(pc) + "  (lower is cheaper)")

    println(nl + "--- One training step ---" + nl)
    var target = [0.5, 0.5, 0.5, 0.5]
    var trained = adaptive_train_step(layers, input_b, target, dim, max_depth, 0.01)
    // last entry is the summary
    println(tab + trained[len(trained) - 1])

    println(nl + "Key insight:" + nl)
    println(tab + "Input D used fewer layers than Input A.")
    println(tab + "In PyTorch every input always uses all layers.")
    println(tab + "Adaptive depth saves compute for easy inputs while" + nl + tab + "reserving depth for hard ones — impossible in a static graph.")
}
