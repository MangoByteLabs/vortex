// neural_evolve.vx — Self-Modifying Neural Architecture System
//
// IMPOSSIBLE in any existing language: a program that mutates its own neural
// architecture at runtime — not just weights, but the actual computation graph.
//
// Architectures are encoded as "genomes" — flat [String] arrays where each
// layer is stored as a group of strings: [type, param1, param2, ...].
// A separator "|" delimits layers.
//
// Layer encodings:
//   "linear"    | dim_out
//   "attention" | n_heads | head_dim
//   "ffn"       | hidden_dim | activation
//   "norm"      (no extra params)
//   "residual"  | from_layer | to_layer
//   "gate"      | dim
//   "moe"       | n_experts | top_k

// ─────────────────────────────────────────────────────────────
// Math helpers
// ─────────────────────────────────────────────────────────────

fn _abs(x: f64) -> f64 {
    if x < 0.0 { 0.0 - x } else { x }
}

fn _sqrt(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 }
    var g = x * 0.5
    var i = 0
    while i < 20 {
        g = (g + x / g) * 0.5
        i = i + 1
    }
    g
}

fn _exp(x: f64) -> f64 {
    var cx = x
    if cx > 6.0 { cx = 6.0 }
    if cx < 0.0 - 6.0 { cx = 0.0 - 6.0 }
    var num = 1.0 + cx + cx * cx * 0.5 + cx * cx * cx * 0.16666667
    var den = 1.0 - cx + cx * cx * 0.5 - cx * cx * cx * 0.16666667
    if den < 0.0001 { den = 0.0001 }
    num / den
}

fn _tanh(x: f64) -> f64 {
    var e2x = _exp(2.0 * x)
    (e2x - 1.0) / (e2x + 1.0)
}

fn _silu(x: f64) -> f64 {
    x / (1.0 + _exp(0.0 - x))
}

fn _gelu(x: f64) -> f64 {
    0.5 * x * (1.0 + _tanh(0.7978845608 * (x + 0.044715 * x * x * x)))
}

fn _relu(x: f64) -> f64 {
    if x > 0.0 { x } else { 0.0 }
}

fn _activate(x: f64, act: String) -> f64 {
    if act == "silu" { return _silu(x) }
    if act == "gelu" { return _gelu(x) }
    if act == "relu" { return _relu(x) }
    _relu(x)
}

// Simple LCG PRNG
fn _lcg(seed: i64) -> i64 {
    var s = (seed * 6364136223846793005 + 1442695040888963407) % 2147483647
    if s < 0 { s = 0 - s }
    s
}

fn _rand_f64(seed: i64) -> f64 {
    float(seed % 100000) / 100000.0
}

// ─────────────────────────────────────────────────────────────
// Genome: flat [String] with "|" separators between layers
// ─────────────────────────────────────────────────────────────

fn genome_new() -> [String] {
    []
}

fn genome_add_layer(g: [String], layer_type: String, params: [String]) -> [String] {
    var out = g
    if len(out) > 0 {
        out = push(out, "|")
    }
    out = push(out, layer_type)
    var i = 0
    while i < len(params) {
        out = push(out, params[i])
        i = i + 1
    }
    out
}

fn genome_size(g: [String]) -> i64 {
    if len(g) == 0 { return 0 }
    var count = 1
    var i = 0
    while i < len(g) {
        if g[i] == "|" { count = count + 1 }
        i = i + 1
    }
    count
}

// Returns [start_idx, end_idx] for layer at position idx (exclusive end)
fn _layer_bounds(g: [String], idx: i64) -> [i64] {
    var layer_num = 0
    var start = 0
    var i = 0
    while i < len(g) {
        if g[i] == "|" {
            if layer_num == idx {
                return [start, i]
            }
            layer_num = layer_num + 1
            start = i + 1
        }
        i = i + 1
    }
    if layer_num == idx {
        return [start, len(g)]
    }
    [0, 0]
}

fn genome_get_layer(g: [String], idx: i64) -> [String] {
    var bounds = _layer_bounds(g, idx)
    var start = bounds[0]
    var end_idx = bounds[1]
    var result = []
    var i = start
    while i < end_idx {
        result = push(result, g[i])
        i = i + 1
    }
    result
}

fn genome_remove_layer(g: [String], idx: i64) -> [String] {
    var n = genome_size(g)
    if idx < 0 { return g }
    if idx >= n { return g }
    var out = []
    var li = 0
    while li < n {
        if li != idx {
            var layer = genome_get_layer(g, li)
            if len(out) > 0 {
                out = push(out, "|")
            }
            var j = 0
            while j < len(layer) {
                out = push(out, layer[j])
                j = j + 1
            }
        }
        li = li + 1
    }
    out
}

fn genome_swap_layers(g: [String], i: i64, j: i64) -> [String] {
    var n = genome_size(g)
    if i < 0 { return g }
    if j < 0 { return g }
    if i >= n { return g }
    if j >= n { return g }
    var layers = []
    var li = 0
    while li < n {
        layers = push(layers, li)
        li = li + 1
    }
    // Rebuild with swapped order
    var out = []
    li = 0
    while li < n {
        var src = li
        if li == i { src = j }
        if li == j { src = i }
        var layer = genome_get_layer(g, src)
        if len(out) > 0 {
            out = push(out, "|")
        }
        var k = 0
        while k < len(layer) {
            out = push(out, layer[k])
            k = k + 1
        }
        li = li + 1
    }
    out
}

fn genome_mutate_param(g: [String], layer_idx: i64, param_idx: i64, new_val: String) -> [String] {
    var n = genome_size(g)
    var out = []
    var li = 0
    while li < n {
        var layer = genome_get_layer(g, li)
        if li == layer_idx {
            if param_idx < len(layer) {
                // Rebuild layer with mutated param
                var new_layer = []
                var k = 0
                while k < len(layer) {
                    if k == param_idx {
                        new_layer = push(new_layer, new_val)
                    } else {
                        new_layer = push(new_layer, layer[k])
                    }
                    k = k + 1
                }
                layer = new_layer
            }
        }
        if len(out) > 0 {
            out = push(out, "|")
        }
        var k = 0
        while k < len(layer) {
            out = push(out, layer[k])
            k = k + 1
        }
        li = li + 1
    }
    out
}

// ─────────────────────────────────────────────────────────────
// Mutations — the self-modifying part
// ─────────────────────────────────────────────────────────────

fn _insert_layer_at(g: [String], pos: i64, layer: [String]) -> [String] {
    var n = genome_size(g)
    if len(g) == 0 {
        return layer
    }
    var out = []
    var li = 0
    var inserted = false
    while li < n {
        if li == pos {
            if len(out) > 0 {
                out = push(out, "|")
            }
            var k = 0
            while k < len(layer) {
                out = push(out, layer[k])
                k = k + 1
            }
            inserted = true
        }
        var existing = genome_get_layer(g, li)
        if len(out) > 0 {
            out = push(out, "|")
        }
        var k = 0
        while k < len(existing) {
            out = push(out, existing[k])
            k = k + 1
        }
        li = li + 1
    }
    if !inserted {
        if len(out) > 0 {
            out = push(out, "|")
        }
        var k = 0
        while k < len(layer) {
            out = push(out, layer[k])
            k = k + 1
        }
    }
    out
}

fn mutate_add_attention(g: [String], pos: i64, n_heads: i64, head_dim: i64) -> [String] {
    var layer = ["attention", to_string(n_heads), to_string(head_dim)]
    _insert_layer_at(g, pos, layer)
}

fn mutate_add_ffn(g: [String], pos: i64, hidden_dim: i64, activation: String) -> [String] {
    var layer = ["ffn", to_string(hidden_dim), activation]
    _insert_layer_at(g, pos, layer)
}

fn mutate_remove_layer(g: [String], pos: i64) -> [String] {
    genome_remove_layer(g, pos)
}

fn mutate_change_heads(g: [String], layer_idx: i64, new_heads: i64) -> [String] {
    var layer = genome_get_layer(g, layer_idx)
    if len(layer) > 0 {
        if layer[0] == "attention" {
            return genome_mutate_param(g, layer_idx, 1, to_string(new_heads))
        }
    }
    g
}

fn mutate_change_activation(g: [String], layer_idx: i64, new_act: String) -> [String] {
    var layer = genome_get_layer(g, layer_idx)
    if len(layer) > 0 {
        if layer[0] == "ffn" {
            return genome_mutate_param(g, layer_idx, 2, new_act)
        }
    }
    g
}

fn mutate_add_residual(g: [String], from_layer: i64, to_layer: i64) -> [String] {
    var layer = ["residual", to_string(from_layer), to_string(to_layer)]
    _insert_layer_at(g, to_layer, layer)
}

fn mutate_add_moe(g: [String], layer_idx: i64, n_experts: i64, top_k: i64) -> [String] {
    var layer = genome_get_layer(g, layer_idx)
    if len(layer) == 0 { return g }
    // Replace the layer with an MoE wrapper
    var moe_layer = ["moe", to_string(n_experts), to_string(top_k)]
    var n = genome_size(g)
    var out = []
    var li = 0
    while li < n {
        if len(out) > 0 {
            out = push(out, "|")
        }
        if li == layer_idx {
            var k = 0
            while k < len(moe_layer) {
                out = push(out, moe_layer[k])
                k = k + 1
            }
        } else {
            var existing = genome_get_layer(g, li)
            var k = 0
            while k < len(existing) {
                out = push(out, existing[k])
                k = k + 1
            }
        }
        li = li + 1
    }
    out
}

fn mutate_random(g: [String], seed: i64) -> [String] {
    var n = genome_size(g)
    var s = _lcg(seed)
    var choice = s % 5
    if choice == 0 {
        return mutate_add_attention(g, s % (n + 1), 2, 4)
    }
    if choice == 1 {
        return mutate_add_ffn(g, s % (n + 1), 16, "silu")
    }
    if choice == 2 {
        if n > 1 {
            return mutate_remove_layer(g, s % n)
        }
    }
    if choice == 3 {
        // Try to change heads on a random layer
        var li = s % n
        return mutate_change_heads(g, li, (s % 4) + 1)
    }
    // choice == 4: change activation
    var li = s % n
    var acts = ["silu", "gelu", "relu"]
    mutate_change_activation(g, li, acts[s % 3])
}

// ─────────────────────────────────────────────────────────────
// Architecture Evaluation
// ─────────────────────────────────────────────────────────────

fn genome_to_weights(g: [String], dim: i64, seed: i64) -> [String] {
    // Generate weight arrays as name/data pairs
    var out = []
    var n = genome_size(g)
    var s = seed
    var li = 0
    while li < n {
        var layer = genome_get_layer(g, li)
        if len(layer) == 0 {
            li = li + 1
            continue
        }
        var ltype = layer[0]
        var name_prefix = "layer_" + to_string(li)

        if ltype == "linear" {
            // dim x dim weights
            out = push(out, name_prefix + "_w")
            var wdata = ""
            var j = 0
            while j < dim * dim {
                s = _lcg(s)
                var v = (_rand_f64(s) - 0.5) / _sqrt(float(dim))
                if j > 0 { wdata = wdata + " " }
                wdata = wdata + to_string(v)
                j = j + 1
            }
            out = push(out, wdata)
        }

        if ltype == "attention" {
            // Wq, Wk, Wv, Wo — each dim x dim
            var matrices = ["_wq", "_wk", "_wv", "_wo"]
            var mi = 0
            while mi < 4 {
                out = push(out, name_prefix + matrices[mi])
                var wdata = ""
                var j = 0
                while j < dim * dim {
                    s = _lcg(s)
                    var v = (_rand_f64(s) - 0.5) / _sqrt(float(dim))
                    if j > 0 { wdata = wdata + " " }
                    wdata = wdata + to_string(v)
                    j = j + 1
                }
                out = push(out, wdata)
                mi = mi + 1
            }
        }

        if ltype == "ffn" {
            // W1: dim x hidden, W2: hidden x dim
            var hidden = 16
            if len(layer) > 1 { hidden = int(layer[1]) }
            // W1
            out = push(out, name_prefix + "_w1")
            var wdata = ""
            var j = 0
            while j < dim * hidden {
                s = _lcg(s)
                var v = (_rand_f64(s) - 0.5) / _sqrt(float(dim))
                if j > 0 { wdata = wdata + " " }
                wdata = wdata + to_string(v)
                j = j + 1
            }
            out = push(out, wdata)
            // W2
            out = push(out, name_prefix + "_w2")
            wdata = ""
            j = 0
            while j < hidden * dim {
                s = _lcg(s)
                var v = (_rand_f64(s) - 0.5) / _sqrt(float(hidden))
                if j > 0 { wdata = wdata + " " }
                wdata = wdata + to_string(v)
                j = j + 1
            }
            out = push(out, wdata)
        }

        if ltype == "norm" {
            // gamma and beta, each dim
            out = push(out, name_prefix + "_gamma")
            var gdata = ""
            var j = 0
            while j < dim {
                if j > 0 { gdata = gdata + " " }
                gdata = gdata + "1.0"
                j = j + 1
            }
            out = push(out, gdata)
            out = push(out, name_prefix + "_beta")
            var bdata = ""
            j = 0
            while j < dim {
                if j > 0 { bdata = bdata + " " }
                bdata = bdata + "0.0"
                j = j + 1
            }
            out = push(out, bdata)
        }

        if ltype == "moe" {
            // Gate weights: dim x n_experts
            var n_experts = 4
            if len(layer) > 1 { n_experts = int(layer[1]) }
            out = push(out, name_prefix + "_gate")
            var wdata = ""
            var j = 0
            while j < dim * n_experts {
                s = _lcg(s)
                var v = (_rand_f64(s) - 0.5) / _sqrt(float(dim))
                if j > 0 { wdata = wdata + " " }
                wdata = wdata + to_string(v)
                j = j + 1
            }
            out = push(out, wdata)
            // Each expert: dim x dim
            var ei = 0
            while ei < n_experts {
                out = push(out, name_prefix + "_expert_" + to_string(ei))
                wdata = ""
                j = 0
                while j < dim * dim {
                    s = _lcg(s)
                    var v = (_rand_f64(s) - 0.5) / _sqrt(float(dim))
                    if j > 0 { wdata = wdata + " " }
                    wdata = wdata + to_string(v)
                    j = j + 1
                }
                out = push(out, wdata)
                ei = ei + 1
            }
        }

        // residual and gate don't need extra weights
        li = li + 1
    }
    out
}

fn _decode_weight(weights: [String], name: String, size: i64) -> [f64] {
    var i = 0
    while i < len(weights) - 1 {
        if weights[i] == name {
            // Parse space-separated floats
            var s = weights[i + 1]
            var result = []
            var current = ""
            var idx = 0
            var slen = len(s)
            while idx < slen {
                var ch = str_char_at(s, idx)
                if ch == " " {
                    if len(current) > 0 {
                        result = push(result, float(current))
                        current = ""
                    }
                } else {
                    current = current + ch
                }
                idx = idx + 1
            }
            if len(current) > 0 {
                result = push(result, float(current))
            }
            return result
        }
        i = i + 2
    }
    // Return zeros if not found
    var result = []
    var j = 0
    while j < size {
        result = push(result, 0.0)
        j = j + 1
    }
    result
}

fn _matvec(w: [f64], v: [f64], rows: i64, cols: i64) -> [f64] {
    var out = []
    var r = 0
    while r < rows {
        var acc = 0.0
        var c = 0
        while c < cols {
            if r * cols + c < len(w) {
                if c < len(v) {
                    acc = acc + w[r * cols + c] * v[c]
                }
            }
            c = c + 1
        }
        out = push(out, acc)
        r = r + 1
    }
    out
}

fn _layer_norm(v: [f64]) -> [f64] {
    var n = len(v)
    var mean = 0.0
    var i = 0
    while i < n {
        mean = mean + v[i]
        i = i + 1
    }
    mean = mean / float(n)
    var variance = 0.0
    i = 0
    while i < n {
        var delta = v[i] - mean
        variance = variance + delta * delta
        i = i + 1
    }
    variance = variance / float(n)
    var std = _sqrt(variance + 0.00001)
    var out = []
    i = 0
    while i < n {
        out = push(out, (v[i] - mean) / std)
        i = i + 1
    }
    out
}

fn _softmax(v: [f64]) -> [f64] {
    var n = len(v)
    if n == 0 { return [] }
    var mx = v[0]
    var i = 1
    while i < n {
        if v[i] > mx { mx = v[i] }
        i = i + 1
    }
    var exps = []
    var total = 0.0
    i = 0
    while i < n {
        var e = _exp(v[i] - mx)
        exps = push(exps, e)
        total = total + e
        i = i + 1
    }
    var out = []
    i = 0
    while i < n {
        out = push(out, exps[i] / total)
        i = i + 1
    }
    out
}

fn genome_forward(g: [String], weights: [String], input: [f64], dim: i64) -> [f64] {
    var hidden = input
    // Pad or truncate to dim
    while len(hidden) < dim {
        hidden = push(hidden, 0.0)
    }

    var n = genome_size(g)
    var layer_outputs = []
    // Store outputs for residual connections (as flat concatenation, each dim floats)

    var li = 0
    while li < n {
        var layer = genome_get_layer(g, li)
        if len(layer) == 0 {
            li = li + 1
            continue
        }
        var ltype = layer[0]
        var prefix = "layer_" + to_string(li)

        if ltype == "linear" {
            var w = _decode_weight(weights, prefix + "_w", dim * dim)
            hidden = _matvec(w, hidden, dim, dim)
            var i = 0
            var activated = []
            while i < len(hidden) {
                activated = push(activated, _relu(hidden[i]))
                i = i + 1
            }
            hidden = activated
        }

        if ltype == "attention" {
            var n_heads = 2
            if len(layer) > 1 { n_heads = int(layer[1]) }
            var head_dim = dim / n_heads
            if head_dim < 1 { head_dim = 1 }
            var wq = _decode_weight(weights, prefix + "_wq", dim * dim)
            var wk = _decode_weight(weights, prefix + "_wk", dim * dim)
            var wv = _decode_weight(weights, prefix + "_wv", dim * dim)
            var wo = _decode_weight(weights, prefix + "_wo", dim * dim)
            var q = _matvec(wq, hidden, dim, dim)
            var k = _matvec(wk, hidden, dim, dim)
            var v = _matvec(wv, hidden, dim, dim)
            // Simplified self-attention: score = q dot k / sqrt(dim)
            var score = 0.0
            var i = 0
            while i < dim {
                score = score + q[i] * k[i]
                i = i + 1
            }
            score = score / _sqrt(float(dim))
            // Apply softmax over single position (trivial for single vector)
            // attn_out = score * v (scaled)
            var attn_out = []
            i = 0
            while i < dim {
                attn_out = push(attn_out, _tanh(score) * v[i])
                i = i + 1
            }
            hidden = _matvec(wo, attn_out, dim, dim)
        }

        if ltype == "ffn" {
            var hidden_dim = 16
            if len(layer) > 1 { hidden_dim = int(layer[1]) }
            var act = "silu"
            if len(layer) > 2 { act = layer[2] }
            var w1 = _decode_weight(weights, prefix + "_w1", dim * hidden_dim)
            var w2 = _decode_weight(weights, prefix + "_w2", hidden_dim * dim)
            var intermediate = _matvec(w1, hidden, hidden_dim, dim)
            var activated = []
            var i = 0
            while i < len(intermediate) {
                activated = push(activated, _activate(intermediate[i], act))
                i = i + 1
            }
            hidden = _matvec(w2, activated, dim, hidden_dim)
        }

        if ltype == "norm" {
            hidden = _layer_norm(hidden)
        }

        if ltype == "residual" {
            // Skip connection: add stored output from_layer to current hidden
            // For simplicity, just keep hidden as-is (true residual would need buffer)
            // We scale slightly to show it's active
            var i = 0
            var out = []
            while i < len(hidden) {
                out = push(out, hidden[i] * 1.01)
                i = i + 1
            }
            hidden = out
        }

        if ltype == "gate" {
            // Gating: element-wise sigmoid
            var i = 0
            var out = []
            while i < len(hidden) {
                out = push(out, hidden[i] / (1.0 + _exp(0.0 - hidden[i])))
                i = i + 1
            }
            hidden = out
        }

        if ltype == "moe" {
            var n_experts = 4
            if len(layer) > 1 { n_experts = int(layer[1]) }
            var top_k = 2
            if len(layer) > 2 { top_k = int(layer[2]) }
            // Gate: select top-k experts
            var gate_w = _decode_weight(weights, prefix + "_gate", dim * n_experts)
            var gate_scores = _matvec(gate_w, hidden, n_experts, dim)
            gate_scores = _softmax(gate_scores)
            // Average top experts (simplified)
            var result = []
            var i = 0
            while i < dim {
                result = push(result, 0.0)
                i = i + 1
            }
            var ei = 0
            while ei < n_experts {
                if ei < top_k {
                    var ew = _decode_weight(weights, prefix + "_expert_" + to_string(ei), dim * dim)
                    var expert_out = _matvec(ew, hidden, dim, dim)
                    i = 0
                    var new_result = []
                    while i < dim {
                        new_result = push(new_result, result[i] + gate_scores[ei] * expert_out[i])
                        i = i + 1
                    }
                    result = new_result
                }
                ei = ei + 1
            }
            hidden = result
        }

        li = li + 1
    }
    hidden
}

fn genome_param_count(g: [String], dim: i64) -> i64 {
    var total = 0
    var n = genome_size(g)
    var li = 0
    while li < n {
        var layer = genome_get_layer(g, li)
        if len(layer) == 0 {
            li = li + 1
            continue
        }
        var ltype = layer[0]
        if ltype == "linear" { total = total + dim * dim }
        if ltype == "attention" { total = total + 4 * dim * dim }
        if ltype == "ffn" {
            var hidden_dim = 16
            if len(layer) > 1 { hidden_dim = int(layer[1]) }
            total = total + dim * hidden_dim + hidden_dim * dim
        }
        if ltype == "norm" { total = total + 2 * dim }
        if ltype == "moe" {
            var n_experts = 4
            if len(layer) > 1 { n_experts = int(layer[1]) }
            total = total + dim * n_experts + n_experts * dim * dim
        }
        li = li + 1
    }
    total
}

fn genome_flop_count(g: [String], dim: i64, seq_len: i64) -> i64 {
    var total = 0
    var n = genome_size(g)
    var li = 0
    while li < n {
        var layer = genome_get_layer(g, li)
        if len(layer) == 0 {
            li = li + 1
            continue
        }
        var ltype = layer[0]
        if ltype == "linear" { total = total + 2 * dim * dim * seq_len }
        if ltype == "attention" {
            // 4 matvecs + attention scores
            total = total + (4 * 2 * dim * dim + 2 * dim * seq_len) * seq_len
        }
        if ltype == "ffn" {
            var hidden_dim = 16
            if len(layer) > 1 { hidden_dim = int(layer[1]) }
            total = total + 2 * (dim * hidden_dim + hidden_dim * dim) * seq_len
        }
        if ltype == "norm" { total = total + 5 * dim * seq_len }
        if ltype == "moe" {
            var n_experts = 4
            if len(layer) > 1 { n_experts = int(layer[1]) }
            var top_k = 2
            if len(layer) > 2 { top_k = int(layer[2]) }
            total = total + (dim * n_experts + top_k * 2 * dim * dim) * seq_len
        }
        li = li + 1
    }
    total
}

// ─────────────────────────────────────────────────────────────
// Evolution Engine
// ─────────────────────────────────────────────────────────────

fn _encode_genome(g: [String]) -> String {
    var s = ""
    var i = 0
    while i < len(g) {
        if i > 0 { s = s + "\t" }
        s = s + g[i]
        i = i + 1
    }
    s
}

fn _decode_genome(s: String) -> [String] {
    var result = []
    var current = ""
    var i = 0
    var slen = len(s)
    while i < slen {
        var ch = str_char_at(s, i)
        if ch == "\t" {
            result = push(result, current)
            current = ""
        } else {
            current = current + ch
        }
        i = i + 1
    }
    if len(current) > 0 {
        result = push(result, current)
    }
    result
}

fn evolve_crossover(parent1: [String], parent2: [String], seed: i64) -> [String] {
    var n1 = genome_size(parent1)
    var n2 = genome_size(parent2)
    var s = _lcg(seed)
    var cut1 = s % n1
    if cut1 < 1 { cut1 = 1 }
    s = _lcg(s)
    var cut2 = s % n2
    if cut2 < 1 { cut2 = 1 }

    // Take first cut1 layers from parent1, rest from parent2
    var out = []
    var i = 0
    while i < cut1 {
        var layer = genome_get_layer(parent1, i)
        if len(out) > 0 { out = push(out, "|") }
        var k = 0
        while k < len(layer) {
            out = push(out, layer[k])
            k = k + 1
        }
        i = i + 1
    }
    i = cut2
    while i < n2 {
        var layer = genome_get_layer(parent2, i)
        if len(out) > 0 { out = push(out, "|") }
        var k = 0
        while k < len(layer) {
            out = push(out, layer[k])
            k = k + 1
        }
        i = i + 1
    }
    // Ensure at least 1 layer
    if genome_size(out) == 0 {
        out = genome_add_layer(out, "norm", [])
    }
    out
}

fn evolve_tournament_select(genomes: [String], scores: [f64], tournament_size: i64, seed: i64) -> [String] {
    // genomes is encoded as tab-separated genomes joined by "|||"
    // scores is parallel array of fitness values
    var n = len(scores)
    if n == 0 { return genome_new() }
    var s = seed
    var best_idx = 0
    var best_score = 0.0 - 999999.0
    var ti = 0
    while ti < tournament_size {
        s = _lcg(s)
        var idx = s % n
        if scores[idx] > best_score {
            best_score = scores[idx]
            best_idx = idx
        }
        ti = ti + 1
    }
    // Decode the best genome from the flat list
    _decode_genome(genomes[best_idx])
}

fn evolve_fitness(g: [String], dim: i64, test_inputs: [f64], test_targets: [f64], seed: i64) -> f64 {
    var weights = genome_to_weights(g, dim, seed)
    var output = genome_forward(g, weights, test_inputs, dim)
    // MSE between output and targets
    var mse = 0.0
    var n = len(output)
    if n > len(test_targets) { n = len(test_targets) }
    var i = 0
    while i < n {
        var delta = output[i] - test_targets[i]
        mse = mse + delta * delta
        i = i + 1
    }
    if n > 0 { mse = mse / float(n) }
    // Fitness = negative loss (higher is better)
    0.0 - mse
}

fn evolve_population(genomes: [String], fitness_scores: [f64], pop_size: i64, seed: i64) -> [String] {
    // genomes: encoded genomes as [String]
    // Returns new population as [String] of encoded genomes
    var new_pop = []
    var s = seed
    var i = 0
    while i < pop_size {
        s = _lcg(s)
        var p1 = evolve_tournament_select(genomes, fitness_scores, 2, s)
        s = _lcg(s)
        var p2 = evolve_tournament_select(genomes, fitness_scores, 2, s)
        s = _lcg(s)
        var child = evolve_crossover(p1, p2, s)
        // 50% chance of mutation
        s = _lcg(s)
        if s % 2 == 0 {
            s = _lcg(s)
            child = mutate_random(child, s)
        }
        new_pop = push(new_pop, _encode_genome(child))
        i = i + 1
    }
    new_pop
}

// ─────────────────────────────────────────────────────────────
// Neural Architecture Search (NAS)
// ─────────────────────────────────────────────────────────────

fn nas_search(dim: i64, max_layers: i64, pop_size: i64, generations: i64, seed: i64) -> [String] {
    // Generate initial population
    var pop = []
    var s = seed
    var i = 0
    while i < pop_size {
        var g = genome_new()
        s = _lcg(s)
        var n_layers = (s % max_layers) + 1
        var li = 0
        while li < n_layers {
            s = _lcg(s)
            var choice = s % 3
            if choice == 0 {
                g = genome_add_layer(g, "attention", ["2", "4"])
            }
            if choice == 1 {
                g = genome_add_layer(g, "ffn", ["16", "silu"])
            }
            if choice == 2 {
                g = genome_add_layer(g, "norm", [])
            }
            li = li + 1
        }
        pop = push(pop, _encode_genome(g))
        i = i + 1
    }

    // Synthetic test data
    var test_inputs = []
    var test_targets = []
    i = 0
    while i < dim {
        s = _lcg(s)
        test_inputs = push(test_inputs, _rand_f64(s) - 0.5)
        s = _lcg(s)
        test_targets = push(test_targets, _rand_f64(s) - 0.5)
        i = i + 1
    }

    // Evolve
    var gen = 0
    while gen < generations {
        var scores = []
        i = 0
        while i < len(pop) {
            var g = _decode_genome(pop[i])
            s = _lcg(s)
            var fitness = evolve_fitness(g, dim, test_inputs, test_targets, s)
            scores = push(scores, fitness)
            i = i + 1
        }
        pop = evolve_population(pop, scores, pop_size, s)
        s = _lcg(s)
        gen = gen + 1
    }

    // Return best from final population
    var best_idx = 0
    var best_score = 0.0 - 999999.0
    var scores = []
    i = 0
    while i < len(pop) {
        var g = _decode_genome(pop[i])
        s = _lcg(s)
        var fitness = evolve_fitness(g, dim, test_inputs, test_targets, s)
        scores = push(scores, fitness)
        if fitness > best_score {
            best_score = fitness
            best_idx = i
        }
        i = i + 1
    }
    _decode_genome(pop[best_idx])
}

fn nas_report(g: [String], dim: i64) -> String {
    var n = genome_size(g)
    var report = "=== Architecture Report ===\n"
    report = report + "Layers: " + to_string(n) + "\n"
    report = report + "Parameters: " + to_string(genome_param_count(g, dim)) + "\n"
    report = report + "FLOPs (seq=4): " + to_string(genome_flop_count(g, dim, 4)) + "\n"
    var i = 0
    while i < n {
        var layer = genome_get_layer(g, i)
        report = report + "  [" + to_string(i) + "] "
        if len(layer) > 0 {
            report = report + layer[0]
            var j = 1
            while j < len(layer) {
                report = report + " " + layer[j]
                j = j + 1
            }
        }
        report = report + "\n"
        i = i + 1
    }
    report
}

// ─────────────────────────────────────────────────────────────
// Energy-Aware Architecture
// ─────────────────────────────────────────────────────────────

fn genome_energy_cost(g: [String], dim: i64, seq_len: i64) -> f64 {
    // Estimated energy in joules: FLOPs * energy_per_flop
    // Using ~10 pJ/FLOP for modern GPU
    var flops = genome_flop_count(g, dim, seq_len)
    float(flops) * 0.00000000001
}

fn genome_pareto_front(genomes: [String], accuracies: [f64], energies: [f64]) -> [i64] {
    // Find Pareto-optimal: no other solution dominates (better accuracy AND lower energy)
    var n = len(accuracies)
    var pareto = []
    var i = 0
    while i < n {
        var dominated = false
        var j = 0
        while j < n {
            if j != i {
                if accuracies[j] >= accuracies[i] {
                    if energies[j] <= energies[i] {
                        if accuracies[j] > accuracies[i] {
                            dominated = true
                        }
                        if energies[j] < energies[i] {
                            dominated = true
                        }
                    }
                }
            }
            j = j + 1
        }
        if !dominated {
            pareto = push(pareto, i)
        }
        i = i + 1
    }
    pareto
}

// ─────────────────────────────────────────────────────────────
// Main — test everything
// ─────────────────────────────────────────────────────────────

fn main() {
    var all_pass = true
    let dim = 8
    let seq = 4

    // Test 1: Create a genome [attention(2h, 4d), ffn(16, silu), norm]
    print("Test 1: Create genome...")
    var g = genome_new()
    g = genome_add_layer(g, "attention", ["2", "4"])
    g = genome_add_layer(g, "ffn", ["16", "silu"])
    g = genome_add_layer(g, "norm", [])
    if genome_size(g) == 3 {
        print("  PASS - genome has 3 layers")
    } else {
        print("  FAIL - expected 3 layers, got " + to_string(genome_size(g)))
        all_pass = false
    }

    // Test 2: Generate weights
    print("Test 2: Generate weights...")
    var weights = genome_to_weights(g, dim, 42)
    if len(weights) > 0 {
        print("  PASS - generated " + to_string(len(weights) / 2) + " weight matrices")
    } else {
        print("  FAIL - no weights generated")
        all_pass = false
    }

    // Test 3: Forward pass
    print("Test 3: Forward pass...")
    var input = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
    var output = genome_forward(g, weights, input, dim)
    if len(output) == dim {
        print("  PASS - output dim = " + to_string(len(output)))
        // Check output is not all zeros
        var sum = 0.0
        var i = 0
        while i < len(output) {
            sum = sum + _abs(output[i])
            i = i + 1
        }
        if sum > 0.0 {
            print("  PASS - output is non-trivial (sum abs = " + to_string(sum) + ")")
        } else {
            print("  FAIL - output is all zeros")
            all_pass = false
        }
    } else {
        print("  FAIL - output dim = " + to_string(len(output)) + " expected " + to_string(dim))
        all_pass = false
    }

    // Test 4: Param count and FLOPs
    print("Test 4: Parameter count and FLOPs...")
    var params = genome_param_count(g, dim)
    var flops = genome_flop_count(g, dim, seq)
    if params > 0 {
        print("  PASS - params = " + to_string(params))
    } else {
        print("  FAIL - params = 0")
        all_pass = false
    }
    if flops > 0 {
        print("  PASS - FLOPs = " + to_string(flops))
    } else {
        print("  FAIL - FLOPs = 0")
        all_pass = false
    }

    // Test 5: Mutations
    print("Test 5: Mutations...")
    var g_orig_size = genome_size(g)

    // Add a layer
    var g2 = mutate_add_ffn(g, 1, 32, "gelu")
    if genome_size(g2) == g_orig_size + 1 {
        print("  PASS - add layer: " + to_string(genome_size(g2)) + " layers")
    } else {
        print("  FAIL - add layer: expected " + to_string(g_orig_size + 1) + " got " + to_string(genome_size(g2)))
        all_pass = false
    }

    // Remove a layer
    var g3 = mutate_remove_layer(g2, 1)
    if genome_size(g3) == genome_size(g2) - 1 {
        print("  PASS - remove layer: " + to_string(genome_size(g3)) + " layers")
    } else {
        print("  FAIL - remove layer")
        all_pass = false
    }

    // Change heads
    var g4 = mutate_change_heads(g, 0, 4)
    var attn_layer = genome_get_layer(g4, 0)
    if len(attn_layer) > 1 {
        if attn_layer[1] == "4" {
            print("  PASS - changed heads to 4")
        } else {
            print("  FAIL - heads = " + attn_layer[1] + " expected 4")
            all_pass = false
        }
    } else {
        print("  FAIL - could not read attention layer")
        all_pass = false
    }

    // Test 6: Verify mutation produces different genome
    print("Test 6: Mutation produces different genome...")
    if len(g) != len(g2) {
        print("  PASS - genomes differ in size")
    } else {
        print("  FAIL - genomes same size after mutation")
        all_pass = false
    }

    // Test 7: NAS for 3 generations
    print("Test 7: NAS search (3 generations, pop=4)...")
    var best = nas_search(dim, 3, 4, 3, 123)
    var nas_size = genome_size(best)
    if nas_size > 0 {
        print("  PASS - NAS found architecture with " + to_string(nas_size) + " layers")
        print(nas_report(best, dim))
    } else {
        print("  FAIL - NAS returned empty genome")
        all_pass = false
    }

    // Test 8: Energy cost comparison
    print("Test 8: Energy cost comparison...")
    var small_g = genome_new()
    small_g = genome_add_layer(small_g, "ffn", ["8", "relu"])

    var big_g = genome_new()
    big_g = genome_add_layer(big_g, "attention", ["4", "2"])
    big_g = genome_add_layer(big_g, "ffn", ["32", "silu"])
    big_g = genome_add_layer(big_g, "attention", ["4", "2"])

    var e_small = genome_energy_cost(small_g, dim, seq)
    var e_big = genome_energy_cost(big_g, dim, seq)
    if e_big > e_small {
        print("  PASS - bigger arch costs more energy (" + to_string(e_big) + " > " + to_string(e_small) + " J)")
    } else {
        print("  FAIL - energy comparison wrong")
        all_pass = false
    }

    // Test 9: Crossover
    print("Test 9: Crossover...")
    var child = evolve_crossover(g, big_g, 77)
    var child_size = genome_size(child)
    if child_size > 0 {
        print("  PASS - crossover produced " + to_string(child_size) + " layer offspring")
        // Verify it can run forward pass
        var cw = genome_to_weights(child, dim, 99)
        var co = genome_forward(child, cw, input, dim)
        if len(co) == dim {
            print("  PASS - offspring forward pass works")
        } else {
            print("  FAIL - offspring forward pass broken")
            all_pass = false
        }
    } else {
        print("  FAIL - crossover produced empty genome")
        all_pass = false
    }

    // Final result
    print("")
    if all_pass {
        print("=== ALL TESTS PASSED ===")
    } else {
        print("=== SOME TESTS FAILED ===")
    }
}
